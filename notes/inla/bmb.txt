"sampling is infinite" ?

"review of linear model" -- shouldn't have an epsilon there




-------------
p. 4  $y \sim \textrm{Exponential family}$

GLMs aren't generally considered hierarchical -- those are
the "vanilla" (exponential family/link function/linear model)
models.  What do you mean by "nested and non-nested models"?

I would say that mixed/hierarchical/multi-level are often
taken as synonyms; they incorporate random effects, and
missing variables can be handled in this framework

p. 4 \exp under point (i)

GAMs: "nonparametric" in the sense of the model for the location.
The classical meaning of "non-parametric" has to do with the
conditional distribution -- rank-based,or (perhaps) kernel-density
based

p. 8 "conditional on"

p. 9 maybe use | instead of / for conditioning?

p. 9 slide is cut off

p. 11 "probability of observing the data that was observed before it was observed" is a little confusing.  Clarify, at least verbally?

p. 11 cut off

maybe skip MCMC stuff, ABC, since we've talked about them or will talk about them?

p. 15 isn't this list a repeat from earlier?

p. 16 why capitalize "Field" but not "random" ?
(4) what is Q? (I assume it's an arbitrary variance-covariance matrix,
but not defined)
(5) remind me/us what the symbol is here, and what the notation is more
generally (i.e., what does this line mean?)

"nuisance parameters"; "hyperparameters"

equations are cut off 

p. 17 "of formulating"

I'm a little confused by the f(j) notation.  What are the u_ji values?
This looks a little like a decomposition into fixed + random effects,
but it's not quite in the standard way of doing it.  Maybe the f(j)
are *known*?

p. 18 
title is repeated between frame title & slide
"scalar"
"state evolution"

p. 19 again, we need a clear def (maybe verbal) of the \theta_{-j} notation

the details of the steps are going to take a lot of care, if you
want to go this deep.   How do we get from step 20 to 21?  That seems
like a big leap -- the rest is just manipulation of conditionals,
but 21 is hard.  Maybe it's explained by the cut-off text?

If you want to do this much derivation it would probably be best
to do it on the blackboard!

p. 23 it's not clear to me why we need to do the numerical exploration --
I thought we could do the Laplace approximation with just knowledge
of the mode and the local curvature?

p. 24 "lack of skewness" refers to the fact that the
posterior *is* skewed and the MVN distribution can't handle it?

p. 32 if this is a cut & paste from the vignette or paper, can 
you run the code yourself to make sure you get the same answers?

p. 33 is there any justification for this example?  Where did it
come from?

p. 35 be prepared to explain the prior differences!
