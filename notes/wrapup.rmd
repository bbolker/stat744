---
title: "Stat 744 synthesis"
author: "Ben Bolker"
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
bibliography: nonlin.bib
output:
   ioslides_presentation
---

## Dynamic models

- subset of *hierarchical models* with sequential structure
- all about conditioning on/integrating over latent variables (unobserved nodes)
- handle high-dimensional integration by approximation or stochastic sampling
- sequential (e.g. Kalman, PF) vs. non-sequential methods (general MC)


## frequentist 'vs.' Bayesian

### frequentist

- estimation: direct MLE (no latent variables), Kalman filter, data cloning, IF2, synthetic likelihood, MCEM
- point estimates: MLE
- inference: Wald, profile confidence intervals (bootstrapping)
- no (top-level) priors
- tools: `pomp`, `synlik`, `NIMBLE`

### Bayesian

- estimation: MCMC (including sequential MC/PF), INLA, ABC
- point estimates: mean, but maybe MAP (maximum *a posteriori*)
- inference: quantiles, HPD intervals
- tools: `JAGS`, `Stan`: also

## what's needed: generally

- forward simulation/likelihood for process (pomp: `rmeasure`/`dmeasure`)
- simulation/likelihood for observation (pomp: `dobservation`/`robservation`)

e.g. 

$$
y_{t+1} \sim N(y_t,\sigma^2)
$$

vs.

$$
{\cal L}(y_{t+1}|y_t,\sigma^2)
$$

"plug and play": only need `rmeasure`, not `dmeasure`

## what's needed: specifically

- ABC, synthetic likelihood: only process + summary statistics
- particle filtering: simulate process, likelihood for (obs|process)
- MCMC, data cloning, MCEM:  
conditional likelihoods of every node



## Hamiltonian MCMC

Three critical tricks:

- Hamiltonian MCMC
- automatic differentiation
- 'no-U-turn' sampler (NUTS)

Implementations in: Stan, TMB, PyMC

## Hamiltonian MCMC

- physical interpretation
    - "position" = set of parameter values
    - "momentum" = velocity*mass (chosen randomly)
    - "kinetic energy"
    - "potential energy": $-\log(P)$
- discrete steps according to Hamiltonian equations
    - reversible, volume-preserving, etc.
    - *automatically* handles correlations among variables
- continuous distributions only
- need gradient calculation

## Hamiltonian MCMC: updates

- need to choose update: "leapfrog"  
(change momentum by $\epsilon/2$, position by $\epsilon$, momentum by $\epsilon/2$)
- still need to pick step size: *No-U-Turn Sampler* (NUTS)

![](HMC1.png)

## general tools

- conjugate sampling
- slice samplers (JAGS, NIMBLE)
- sequential Monte Carlo (particle MCMC, IF2, sequential ABC)
- Metropolis-Hastings (JAGS, NIMBLE)
- cooling/contraction (simulated annealing, data cloning, IF2)
- Laplace approximation (INLA, TMB)
- Hamiltonian MCMC (Stan: see below)

Mix and match!

## Software

- formula interface (INLA, `dynlm`, Kalman filters) vs. write-your-own model
- write-your-own:
     - BUGS language (JAGS, NIMBLE, data cloning)
     - variations on C++ (Stan, TMB)
     - R or C++ (`synlik`, `abc`)
- limitations of software
     - broader classes of models $\to$ write-your-own
- value of software diversity/multiple implementations

## limiting factors

- data
- algorithms
- implementations
- computation time

Need *general* building blocks ...
